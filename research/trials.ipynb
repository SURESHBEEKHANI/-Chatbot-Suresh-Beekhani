{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Chatbot-Suresh-Beekhani\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Chatbot-Suresh-Beekhani'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # For splitting text into chunks\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # HuggingFace embedding model for converting text to vectors\n",
    "from langchain_qdrant import QdrantVectorStore  # type: ignore # For storing vectors in Qdrant # To load documents from the web (correct import statement)\n",
    "import os  # Standard library for interacting with the operating system\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qdrant_api_key=os.environ.get('qdrant_api_key') \n",
    "groq_API_KEY=os.environ.get('groq_API_KEY')\n",
    "qdrant_url = \"https://e92b3638-9cd8-43e3-9c5d-a049f560fab2.us-east4-0.gcp.cloud.qdrant.io\"\n",
    "collection_name = \"skchatbot\"\n",
    "os.environ[\"groq_API_KEY\"] = groq_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_split_pdfs(folder_path, chunk_size, chunk_overlap):\n",
    "    \"\"\"Load PDFs from folder and split into chunks with custom separators.\"\"\"\n",
    "    \n",
    "    # Load PDF files from the folder\n",
    "    loader = DirectoryLoader(folder_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Define custom separators for splitting text\n",
    "    separators = [\n",
    "        \"\\n\\n\",  # Paragraph break\n",
    "        \"\\n\",    # Line break\n",
    "        \" \",     # Space\n",
    "        \".\",     # Full stop\n",
    "        \",\",     # Comma\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\"        # Empty separator\n",
    "    ]\n",
    "\n",
    "    # Create a text splitter with the given chunk size, overlap, and separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators\n",
    "    )\n",
    "    \n",
    "    # Split the documents into smaller chunks\n",
    "    return text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:/Chatbot-Suresh-Beekhani/Data\"  # Update to your folder path\n",
    "\n",
    "\n",
    "# Call the function with the folder path\n",
    "doc_chunks = load_and_split_pdfs(folder_path, chunk_size=500, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content=\"I am an accomplished Machine Learning Engineer with a robust foundation in AI, data \\nanalysis, and software development. With a Bachelor's degree in Computer Science from Sir \\nSyed University of Engineering & Technology (SSUET) and certiﬁcations in machine learning, \\nPython, SQL, and data science, I specialize in creating innovative solutions that leverage \\ncutting-edge technologies. My expertise spans predictive modeling, natural language\"),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content='processing (NLP), computer vision, deep learning, and generative AI.\\nIn my professional journey, I have worked with renowned platforms such as Fiverr and \\nUpwork, delivering custom AI and machine learning solutions to a global clientele. I have also \\nserved as an Associate Machine Learning Engineer at Avanza Solutions, leading advanced AI \\nprojects that transformed operations and delivered actionable insights. My technical skill set'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content='includes Python programming, LangChain, large language models (LLMs), and Streamlit, \\nenabling me to build scalable, impactful applications.\\nProfessional Experience:\\nKey Skills:\\nCertiﬁcations:\\nGenerative AI Engineer (Dec 2023 - Present)\\nFreelancer (Fiverr)\\nDeveloped tailored AI applications using advanced tools for predictive modeling, NLP, \\nand generative AI. Delivered innovative solutions to meet unique client requirements.\\nMachine Learning Engineer (Dec 2022 - Present)\\nFreelancer (Upwork)'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content='Successfully completed machine learning and data analysis projects, utilizing Python, \\nLangChain, and other advanced frameworks for e\\x00cient and customized solutions.\\nAssociate Machine Learning Engineer (Dec 2022 - Dec 2023)\\nAvanza Solutions (Hybrid, Karachi, Pakistan)\\nSpearheaded AI and machine learning projects, delivering innovative solutions to \\nimprove operational e\\x00ciency and extract meaningful insights from data.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content='Machine Learning & AI: Expertise in NLP, predictive modeling, and computer vision.\\nGenerative AI: Hands-on experience with LangChain, LLMs, and AI-driven automation \\ntools.\\nPython Development: Proﬁcient in TensorFlow, PyTorch, Pandas, and Streamlit.\\nData Analysis & Visualization: Creating interactive visualizations and deriving insights \\nfrom complex datasets.\\nWeb Development: Skilled in Flask and Streamlit for building user-friendly interfaces.\\nSQL: Advanced querying and database management.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 0}, page_content='Kaggle – Machine Learning (Sep 2023)\\nHackerRank – Basics of SQL (Jun 2023)\\nHackerRank – Python Basics (Jun 2023)\\nIBM – What is Data Science? (Jun 2023)'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 1}, page_content='Notable Projects\\nConversational SQL Assistant Powered by AI\\nJune 2024\\nAI ATS Tracking System\\nMay 2024\\nAI-Powered News Research Tool\\nApril 2024\\nHeart Attack Prediction\\nJune 2023\\nStudent Exam Performance Prediction\\nMay 2023\\nGemstone Price Prediction\\nApril 2023\\nBuilt a conversational SQL assistant using Streamlit, PostgreSQL, and AI models for \\ndynamic SQL query generation from natural language inputs.\\nKey Features: Chat-based interface, AI-driven SQL creation, natural language responses,'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 1}, page_content='and secure PostgreSQL connection.\\nDeveloped an AI-powered resume evaluation tool that matches resumes with job \\ndescriptions, providing scores and actionable feedback.\\nTechnologies Used: Streamlit and Gemini-1.5-Flash.\\nCreated a research tool for e\\x00cient news article analysis.\\nKey Features: Content extraction, AI-powered querying, searchable embeddings, and \\ninteractive chat interface using ChatGroq AI and Google Generative AI Embeddings.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 1}, page_content='Machine learning tool for assessing heart attack risk based on medical indicators like \\ncholesterol levels, blood pressure, and ECG results.\\nKey Features: Logistic regression, decision trees, and a web-based user interface for \\ninstant feedback.\\nPredicted math scores based on demographic and academic factors to support \\npersonalized interventions for students.\\nTechnologies Used: Python, machine learning models, and data visualization.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 1}, page_content='Developed a prediction tool for gemstone prices based on features like cut, clarity, and \\ncarat.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 2}, page_content='Text Generation Using LSTM\\nJanuary 2023\\nImplementation of Autoencoders\\nDecember 2022\\nMovie Recommender System\\nMarch 2022\\nTechnologies Used: Regression models and feature engineering.\\nBuilt an LSTM-based text generation system to predict the next word in a sequence using \\nsynthetic data.\\nKey Features: Template-based sentence generation, tokenization, and one-hot encoding.\\nExplored vanilla, denoising, convolutional, and variational autoencoders for tasks like'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\Data\\\\Machine Learning Engineer .pdf', 'page': 2}, page_content='data compression, denoising, and anomaly detection.\\nApplications: Image processing and sales data analysis.\\nDesigned a personalized recommendation engine combining metadata and textual \\nfeatures.\\nKey Features: Textual analysis with CountVectorizer and Cosine Similarity for tailored \\nmovie suggestions.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SURESH BEEKHANI\\AppData\\Local\\Temp\\ipykernel_16056\\1255207922.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
      "c:\\conda_envs\\mychatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QdrantVectorStore with documents and embedding model\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    documents=doc_chunks,              # List of Document objects to be stored in the vector store\n",
    "    embedding=embeddings,              # Embedding model used to convert documents into vectors\n",
    "    url=qdrant_url,                    # URL for the Qdrant service\n",
    "    api_key=qdrant_api_key,            # API key for accessing the Qdrant service\n",
    "    collection_name=collection_name    # Name of the collection to store the vectors in\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name= collection_name,\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Suresh beekhani ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Data\\\\Suresh Beekhani.pdf', 'page': 0, '_id': '1846b96e-c7f0-4b1d-9397-5e9b3c5a85ec', '_collection_name': 'skchatbot'}, page_content='Suresh Beekhani - A Passionate and Skilled Machine Learning Engineer\\nSuresh Beekhani is a highly motivated and experienced Machine Learning \\nEngineer with over two years of experience in developing and deploying end-to-\\nend machine learning solutions. He possesses a strong foundation in data \\nscience, machine learning, deep learning, and natural language processing \\n(NLP). Suresh is passionate about leveraging the power of AI to extract'),\n",
       " Document(metadata={'source': 'Data\\\\Suresh Beekhani.pdf', 'page': 1, '_id': 'dd064ec6-8801-4bfd-ae51-48b66240a6cc', '_collection_name': 'skchatbot'}, page_content=\"Education and Continuous Learning:\\nSuresh holds a Bachelor's degree in Computer Science and continuously \\nupdates his skills through online courses and certiﬁcations in areas like Python, \\nSQL, and Data Science. \\nA Collaborative and Results-Oriented Individual:\\nSuresh is a highly collaborative and results-oriented individual with excellent \\ncommunication and problem-solving skills. He is eager to contribute his \\nexpertise and passion to challenging projects and make a signiﬁcant impact in\"),\n",
       " Document(metadata={'source': 'Data\\\\Suresh Beekhani.pdf', 'page': 0, '_id': 'e7ebea5a-ccf2-4598-8bcd-bbe4859e5d3f', '_collection_name': 'skchatbot'}, page_content='meaningful insights from data and solve complex business challenges. \\nKey Skills and Expertise:\\nProfessional Experience:\\nSuresh has a proven track record of success in various roles, including:\\nProjects:\\nSuresh has completed several impressive projects that demonstrate his skills \\nand creativity, such as:\\nMachine Learning: Proﬁcient in building predictive models, performing data \\nanalysis, and developing machine learning algorithms.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    api_key=groq_API_KEY,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system instruction\n",
    "system_prompt = (\n",
    "    \"You are a concise and informative question-answering assistant. \"\n",
    "    \"Provide accurate answers using the given context. \"\n",
    "    \"If the answer is unknown, acknowledge it honestly. \"\n",
    "    \"Keep responses brief, ideally within three sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suresh Beekhani is a highly motivated and experienced Machine Learning Engineer with over two years of experience in developing and deploying end-to-end machine learning solutions. He has a strong foundation in data science, machine learning, deep learning, and natural language processing (NLP). Suresh is passionate about leveraging AI to extract meaningful insights from data and solve complex business challenges.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Who is Suresh beekhan\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
