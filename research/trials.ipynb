{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Chatbot-Suresh-Beekhani\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # For splitting text into chunks\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # HuggingFace embedding model for converting text to vectors\n",
    "from langchain_qdrant import QdrantVectorStore  # type: ignore # For storing vectors in Qdrant # To load documents from the web (correct import statement)\n",
    "import os  # Standard library for interacting with the operating system\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qdrant_api_key=os.environ.get('qdrant_api_key') \n",
    "os.environ[\"qdrant_api_key\"] = qdrant_api_key\n",
    "groq_API_KEY=os.environ.get('groq_API_KEY')\n",
    "qdrant_url = \"https://e92b3638-9cd8-43e3-9c5d-a049f560fab2.us-east4-0.gcp.cloud.qdrant.io\"\n",
    "collection_name = \"skchatbot\"\n",
    "os.environ[\"groq_API_KEY\"] = groq_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_split_pdfs(file_path, chunk_size, chunk_overlap):\n",
    "    \"\"\"Load a single PDF and split into chunks with custom separators.\"\"\"\n",
    "    \n",
    "    # Load the single PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Define custom separators for splitting text\n",
    "    separators = [\n",
    "        \"\\n\\n\",  # Paragraph break\n",
    "        \"\\n\",    # Line break\n",
    "        \" \",     # Space\n",
    "        \".\",     # Full stop\n",
    "        \",\",     # Comma\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\"        # Empty separator\n",
    "    ]\n",
    "\n",
    "    # Create a text splitter with the given chunk size, overlap, and separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators\n",
    "    )\n",
    "    \n",
    "    # Split the documents into smaller chunks\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Example of calling the function with the path to a single PDF file\n",
    "file_path = r\"D:\\Chatbot-Suresh-Beekhani\\data\\surseh.pdf\"  # Path to your single PDF file\n",
    "doc_chunks = load_and_split_pdfs(file_path, chunk_size=500, chunk_overlap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0}, page_content=\"Suresh Beekhani - Data Scientist and AI/ML Engineer\\nOverview:\\nSuresh Beekhani is a Data Scientist and AI/ML Engineer with over 3.5 years of industry \\nexperience, specializing in Artiﬁcial Intelligence (AI), Generative AI, Natural Language \\nProcessing (NLP), and Machine Learning (ML). He holds a Bachelor's degree in Computer \\nScience and has extensive experience in developing and deploying machine learning models \\nacross various industries.\\nProfessional Experience:\\nSkills and Expertise:\"),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0}, page_content='Generative AI Engineer at Fiverr (Dec 2023 – Present):\\nSuresh collaborates with global clients, providing expertise in predictive modeling, NLP, \\ncomputer vision, and deep learning. He is proﬁcient in Python, working with Large \\nLanguage Models (LLMs), LLMOps, vector databases, and frameworks like LangChain and \\nLangGraph.\\nMachine Learning Engineer at Upwork (Dec 2022 – Present):\\nSuresh has successfully undertaken a variety of machine learning projects, specializing in'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0}, page_content='predictive modeling, NLP, computer vision, and deep learning, delivering solutions \\ntailored to client requirements.\\nAssociate Machine Learning Engineer at Avanza Solutions (Dec 2022 – Dec 2023):\\nSuresh led AI and ML projects, working in a hybrid environment, to develop cutting-edge \\nsolutions that enhanced operational e\\x00ciencies and provided valuable insights.\\nMachine Learning & Deep Learning:\\nExpertise in building and deploying models for various applications, including predictive'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0}, page_content='modeling, NLP, and computer vision.\\nNatural Language Processing (NLP):\\nProﬁcient in text analysis, sentiment analysis, and language generation using frameworks \\nlike spaCy, Hugging Face, LangChain, and LangGraph.\\nGenerative AI:\\nExperienced in creating AI models for generating new content, including text, images, and \\nmore.\\nAI Agentic Systems:\\nSkilled in developing AI agents capable of autonomous decision-making and task \\nexecution.\\nModel Deployment:'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0}, page_content='Expertise in deploying and managing machine learning models using tools like Docker, \\nAWS services (SageMaker, Lambda, EC2), and FastAPI.\\nMLOps & Cloud Services:\\nKnowledgeable in deploying models and managing lifecycle operations in cloud \\nenvironments like AWS using EC2, ECR, S3, EKS, and API Gateway.\\nLangGraph & LangFlow:\\nProﬁcient in building and visualizing language models using LangGraph and in building \\nand deploying language models using LangFlow.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 1}, page_content='Key Projects:\\nData Management:\\nSkilled in data management and analysis using Phidata to ensure streamlined data \\nprocesses and e\\x00cient analysis workﬂows.\\nCollaborative AI Development:\\nExperienced in developing collaborative AI solutions using CrewaI, enabling the \\nintegration of multiple AI systems for comprehensive tasks.\\n1.\\xa0 Medical-Chatbot-RAG-System:\\nAn AI-powered chatbot that leverages Retrieval Augmented Generation (RAG) to provide'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 1}, page_content='accurate and up-to-date medical information. It uses advanced language models like \\nLlama 3.3 70B and integrates with databases for real-time information retrieval.\\n2.\\xa0WhatsApp Conversations Analysis:\\nA web application that analyzes WhatsApp chat histories, providing insights into message \\nactivity, word usage, media sharing, and user behavior.\\n3.\\xa0 Automated Job Extraction & Cold Email Generator:\\nAutomates the extraction of job postings and generates personalized cold emails for'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 1}, page_content='potential clients using AI.\\n4.\\xa0 AI Quick Summarization:\\nAn AI-powered app that creates summaries from various ﬁle types (PDFs, images, text) \\nusing Google Gemini AI.\\n5.\\xa0 Conversational SQL Assistant:\\nSimpliﬁes database querying by allowing users to interact with databases using natural \\nlanguage.\\n6.\\xa0 AI ATS Tracking System:\\nAn AI-powered app that evaluates resumes against job descriptions, providing match \\nscores and feedback.\\n7.\\xa0 AI-Powered News Research Tool:'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 1}, page_content='Enables e\\x00cient analysis of news articles, allowing users to ask questions and receive \\ndetailed answers using AI.\\n8.\\xa0 Loan Eligibility Prediction:\\nPredicts loan eligibility using machine learning models.\\n9.\\xa0 Heart Attack Prediction:\\nPredicts the risk of heart attacks based on medical indicators.\\n10.\\xa0Student Exam Performance Prediction:\\nPredicts student math scores based on various factors.\\n11.\\xa0Gemstone Price Prediction:\\nPredicts the price of gemstones using machine learning.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 1}, page_content='12.\\xa0Text Generation using LSTM:\\nGenerates text using a Long Short-Term Memory (LSTM) model.\\n13.\\xa0Autoencoder Implementations:\\nExplores and implements various types of autoencoders for tasks like data compression \\nand anomaly detection.\\n14.\\xa0Movie Recommender System:\\nRecommends movies based on user preferences and movie metadata.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 2}, page_content='Common Themes:\\nSkills Demonstrated:\\nServices O\\x00ered:\\nAI/Machine Learning: Many projects utilize AI and machine learning techniques, including \\nnatural language processing (NLP), deep learning, and predictive modeling.\\nStreamlit: Frequently used to create user-friendly web interfaces for the applications.\\nData Analysis: Several projects involve data analysis and visualization, often using \\nPython libraries like Pandas and NumPy.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 2}, page_content='Real-World Applications: The projects demonstrate the application of AI and machine \\nlearning to solve real-world problems in healthcare, ﬁnance, education, and other \\ndomains.\\nPython: Primary programming language used throughout the projects.\\nAI/ML Libraries: Extensive use of TensorFlow, PyTorch, Scikit-learn, and LangChain.\\nData Science Techniques: Proﬁcient in data preprocessing, feature engineering, model \\ntraining, and evaluation.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 2}, page_content='Web Development: Using Streamlit for building interactive web applications.\\nNLP: Techniques like text processing, sentiment analysis, and topic modeling are \\nemployed.\\nMachine Learning & Deep Learning Solutions:\\nCustom ML and DL models developed and deployed to address speciﬁc business \\nchallenges.\\nNatural Language Processing (NLP):\\nImplementation of text analysis, sentiment analysis, and language generation.\\nGenerative AI Applications:'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 2}, page_content='Creation of AI models capable of generating new content (text, images, etc.).\\nAI Agentic Systems:\\nDevelopment of autonomous AI agents for decision-making and task execution.\\nModel Deployment & Management:\\nDeployment of ML models using AWS services, Docker, FastAPI, and Flask for scalability \\nand reliability.\\nChatbot Development:\\nDesign and implementation of intelligent chatbots for customer engagement and \\nsupport.\\nRetrieval-Augmented Generation (RAG):'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 2}, page_content='Integration of RAG techniques to enhance AI model outputs by referencing external \\nknowledge bases.\\nLangGraph & LangFlow Solutions:\\nBuilding, visualizing, and deploying advanced language models using LangGraph and \\nLangFlow.\\nCollaborative AI Development:\\nDeveloping collaborative AI solutions using CrewaI for integrated, multi-agent systems.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 3}, page_content='Certiﬁcations:\\nFor more information or to discuss potential collaborations, please contact Suresh Beekhani \\nat sureshbeekhani26@gmail.com or call +92 3401213187.\\nData Management with Phidata:\\nE\\x00cient data management and analysis solutions using Phidata to ensure high-quality \\ndata processes.\\nIntro to Machine Learning: Completed on Kaggle.\\nBasics of SQL: Cleared the assessment on HackerRank.\\nPython Basics: Mastered the foundational concepts of Python.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 3}, page_content='What is Data Science?: Completed the IBM course on Coursera.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Chatbot-Suresh-Beekhani\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Get the API key from environment variables\n",
    "api_key = os.getenv('google_api_key')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "def get_google_embeddings():\n",
    "    \"\"\"Download and return the Google Generative AI embeddings.\"\"\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_google_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 768\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QdrantVectorStore\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    documents=doc_chunks,              # List of Document objects to be stored in the vector store\n",
    "    embedding=embeddings,              # Embedding model used to convert documents into vectors\n",
    "    url=qdrant_url,                    # URL for the Qdrant service\n",
    "    api_key=qdrant_api_key,            # API key for accessing the Qdrant service\n",
    "    collection_name=collection_name    # Name of the collection to store the vectors in\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name= collection_name,\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Suresh beekhani ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0, '_id': '38ccfc87-70eb-4f5b-bb36-595131beb73c', '_collection_name': 'skchatbot'}, page_content=\"Suresh Beekhani - Data Scientist and AI/ML Engineer\\nOverview:\\nSuresh Beekhani is a Data Scientist and AI/ML Engineer with over 3.5 years of industry \\nexperience, specializing in Artiﬁcial Intelligence (AI), Generative AI, Natural Language \\nProcessing (NLP), and Machine Learning (ML). He holds a Bachelor's degree in Computer \\nScience and has extensive experience in developing and deploying machine learning models \\nacross various industries.\\nProfessional Experience:\\nSkills and Expertise:\"),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 3, '_id': '73d07453-2d48-4916-a352-66426dd2622b', '_collection_name': 'skchatbot'}, page_content='Certiﬁcations:\\nFor more information or to discuss potential collaborations, please contact Suresh Beekhani \\nat sureshbeekhani26@gmail.com or call +92 3401213187.\\nData Management with Phidata:\\nE\\x00cient data management and analysis solutions using Phidata to ensure high-quality \\ndata processes.\\nIntro to Machine Learning: Completed on Kaggle.\\nBasics of SQL: Cleared the assessment on HackerRank.\\nPython Basics: Mastered the foundational concepts of Python.'),\n",
       " Document(metadata={'source': 'D:\\\\Chatbot-Suresh-Beekhani\\\\data\\\\surseh.pdf', 'page': 0, '_id': 'c2302e8b-c2a2-4c2c-afec-c2b2264ef069', '_collection_name': 'skchatbot'}, page_content='Generative AI Engineer at Fiverr (Dec 2023 – Present):\\nSuresh collaborates with global clients, providing expertise in predictive modeling, NLP, \\ncomputer vision, and deep learning. He is proﬁcient in Python, working with Large \\nLanguage Models (LLMs), LLMOps, vector databases, and frameworks like LangChain and \\nLangGraph.\\nMachine Learning Engineer at Upwork (Dec 2022 – Present):\\nSuresh has successfully undertaken a variety of machine learning projects, specializing in')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    api_key=groq_API_KEY,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system instruction\n",
    "system_prompt = (\n",
    "    \"You are a concise and informative question-answering assistant. \"\n",
    "    \"Provide accurate answers using the given context. \"\n",
    "    \"If the answer is unknown, acknowledge it honestly. \"\n",
    "    \"Keep responses brief, ideally within three sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suresh Beekhani is a Data Scientist and AI/ML Engineer with over 3.5 years of industry experience, specializing in Artificial Intelligence, Generative AI, Natural Language Processing, and Machine Learning. He has a Bachelor's degree in Computer Science and has worked on various projects across different industries. He is currently working as a Generative AI Engineer at Fiverr and a Machine Learning Engineer at Upwork.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Who is Suresh beekhan\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
